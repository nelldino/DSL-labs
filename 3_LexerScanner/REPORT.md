#  Lexer & Scanner

### Course: Formal Languages & Finite Automata
### Author: Nelli Garbuz

----

## Overview

A lexer, short for lexical analyzer, is a fundamental component of a compiler or interpreter that breaks down the input text into a sequence of tokens. Its primary role is to scan the input stream of characters and recognize the individual lexical units, which are the smallest meaningful components of the language, such as keywords, identifiers, literals, operators, and punctuation symbols.

The lexer operates in a step-by-step manner, reading characters from the input stream and matching them against predefined patterns to identify tokens. It skips over whitespace and comments, and when it identifies a token, it creates a token object containing information about the token type and possibly its value. The lexer then moves to the next part of the input to continue tokenizing.

A token is a basic unit of lexical analysis generated by the lexer. It represents a meaningful piece of information in the input text. Tokens serve as the building blocks for further stages of the compilation or interpretation process. [1]

## Objectives:

1. Understand what lexical analysis [1] is.

2. Get familiar with the inner workings of a lexer/scanner/tokenizer.

3. Implement a sample lexer and show how it works.

## Implementation description

### **Token Types**

 This section defines various token types used by the lexer. These include keywords, variables, strings, numbers, operators, and special characters.

        KEYWORD = 'KEYWORD'
        VARIABLE = 'VARIABLE'
        STRING = 'STRING'
        NUMBER = 'NUMBER'
        EQUALS = 'EQUALS'
        COMMA = 'COMMA'
        LPAREN = 'LPAREN'
        RPAREN = 'RPAREN'
        LBRACKET = 'LBRACKET'
        RBRACKET = 'RBRACKET'
        NEWLINE = 'NEWLINE'
        EOF = 'EOF'

### **Token Class**
This is a simple class representing a token. It has two attributes: type, which stores the type of the token, and value, which stores the actual value of the token.

    class Token:
    def __init__(self, type, value):
        self.type = type
        self.value = value

    def __str__(self):
        return f'Token({self.type}, {self.value})'

    def __repr__(self):
        return self.__str__()

### **Lexer Class**

 This is the main lexer class responsible for tokenizing the input text.

 **__init__:** Initializes the lexer with the input text and sets the initial position to 0.

 **error:** Raises an exception when encountering an invalid character.

**advance:** Moves the lexer to the next character in the input text.

**skip_whitespace:** Skips over any whitespace characters.

**integer:** Scans and returns an integer from the input text.

**string:** Scans and returns a string enclosed in double quotes from the input text.

**variable:** Scans and returns a variable name consisting of alphanumeric characters.


### **Tokenization Method**

This method iterates through the input text, character by character, and generates tokens based on the current character and its context. It recognizes various token types such as numbers, strings, variables, keywords, operators, and special characters.
    
    def get_next_token(self):
        while self.current_char is not None:
            if self.current_char.isspace():
                self.skip_whitespace()
                continue
### **Example**

This part demonstrates how to use the lexer by tokenizing a sample email automation command represented as a multi-line string. It creates an instance of the Lexer class with the input text and then iterates through the generated tokens, printing each one until the end of the input is reached.

     text = """
     send(
          email="dcretu@example.com",
          cc=["cc1@example.com", "cc2@example.com"],
          bcc=["bcc1@example.com", "bcc2@example.com"],
          subject="Important Email",
          body="LFA laboratory work 3"
          )
    attach(file="lab_report.pdf")
    template(name="report_template")
    """

## Conclusions / Screenshots / Results

**Results**

The result represents the tokens generated by the lexer when tokenizing the input text. Each token consists of a type and a value. Each token type (KEYWORD, LPAREN, RPAREN, EQUALS, COMMA, STRING, etc.) represents a different category of lexical element in the input text. The value of each token represents the actual content of that element. 

For example:

**Token(KEYWORD, 'send'):** Represents the keyword 'send'.

**Token(RBRACKET, ']'):** Represents the right square bracket ']', and so on for other tokens

<img width="222" alt="image" src="https://github.com/nelldino/DSL-labs/assets/120444803/7b7d21cb-bb0a-4b53-9df0-7d1a88a35f92">
<img width="241" alt="image" src="https://github.com/nelldino/DSL-labs/assets/120444803/4c7f24e9-2120-4f57-8ccf-ff453b829526">

## References

[1] https://www.tutorialspoint.com/compiler_design/compiler_design_lexical_analysis.htm

#  Lexer & Scanner

### Course: Formal Languages & Finite Automata
### Author: Nelli Garbuz

----

## Overview

A lexer, short for lexical analyzer, is a fundamental component of a compiler or interpreter that breaks down the input text into a sequence of tokens. Its primary role is to scan the input stream of characters and recognize the individual lexical units, which are the smallest meaningful components of the language, such as keywords, identifiers, literals, operators, and punctuation symbols.

The lexer operates in a step-by-step manner, reading characters from the input stream and matching them against predefined patterns to identify tokens. It skips over whitespace and comments, and when it identifies a token, it creates a token object containing information about the token type and possibly its value. The lexer then moves to the next part of the input to continue tokenizing.

A token is a basic unit of lexical analysis generated by the lexer. It represents a meaningful piece of information in the input text. Tokens serve as the building blocks for further stages of the compilation or interpretation process. [1]

## Objectives:

1. Understand what lexical analysis [1] is.

2. Get familiar with the inner workings of a lexer/scanner/tokenizer.

3. Implement a sample lexer and show how it works.

## Implementation description

In this laboratory work, I created a lexer for parsing a simplified domain-specific language (DSL) used for constructing email messages. this code serves as a lexer for a simplified DSL used to define email messages, enabling users to specify various attributes such as recipients, subject, and body content in a structured format.
### **Token Types**

 This section defines various token types used by the lexer. These include keywords, variables, strings, numbers, operators, and special characters.

        KEYWORD = 'KEYWORD'
        VARIABLE = 'VARIABLE'
        STRING = 'STRING'
        NUMBER = 'NUMBER'
        EQUALS = 'EQUALS'
        COMMA = 'COMMA'
        LPAREN = 'LPAREN'
        RPAREN = 'RPAREN'
        LBRACKET = 'LBRACKET'
        RBRACKET = 'RBRACKET'
        NEWLINE = 'NEWLINE'
        EOF = 'EOF'

**Email Regex**
The EMAIL_PATTERN variable defines a regular expression pattern used to validate email addresses. Here's a breakdown of the pattern:

     EMAIL_PATTERN = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
     
 **[a-zA-Z0-9._%+-]+:** Matches one or more characters from the set [a-zA-Z0-9._%+-], which includes letters (both uppercase and lowercase), digits, and special characters commonly found in email addresses such as period (.)
 
  **@:** Matches the "@" symbol, which separates the local part (username) from the domain part of the email address.
  
**[a-zA-Z0-9.-]+:** Matches one or more characters from the set [a-zA-Z0-9.-] in the domain part of the email address. This includes letters, digits, period, and hyphen.

**\.:** Matches a literal dot (.). The backslash \ is used to a literal dot.

**[a-zA-Z]{2,}**: Matches two or more letters (uppercase or lowercase) at the end of the email address.

**$:** Asserts the end of the string.

Overall, this regular expression pattern matches most standard email addresses. It ensures that the email address follows common conventions for local and domain parts, including the presence of an "@" symbol and a valid domain suffix.

### **Token Class**
This is a simple class representing a token. It has two attributes: type, which stores the type of the token, and value, which stores the actual value of the token.

    class Token:
    def __init__(self, type, value):
        self.type = type
        self.value = value

    def __str__(self):
        return f'Token({self.type}, {self.value})'

    def __repr__(self):
        return self.__str__()

### **Lexer Class**

 This is the main lexer class responsible for tokenizing the input text.

 **__init__:** Initializes the lexer with the input text and sets the initial position to 0. It includes a list of keywords relevant to the email automation.
 
     def __init__(self, text):
        self.text = text
        self.pos = 0
        self.current_char = self.text[self.pos]
        self.keywords = ['send', 'attach', 'template', 'cc', 'bcc', 'subject', 'body']


 **error:** Raises an exception when encountering an invalid character.
 
The error method raises an exception when an invalid character is encountered during tokenization. This helps to provide meaningful feedback if the input text contains syntax errors or unexpected characters.

    def error(self):
        raise Exception('Invalid character')
        
**advance:** Moves the lexer to the next character in the input text.

The advance method increments the current position (pos) in the input text and updates the current_char to the next character in the text. If the end of the text is reached, current_char is set to None

    def advance(self):
        self.pos += 1
        if self.pos < len(self.text):
            self.current_char = self.text[self.pos]
        else:
            self.current_char = None

**skip_whitespace:** Skips over any whitespace characters.

If a whitespace character is encountered, **skip_whitespace()** method is called to advance the position until the next non-whitespace character is found. This is useful for ignoring whitespace between tokens.

    def skip_whitespace(self):
        while self.current_char is not None and self.current_char.isspace():
            self.advance()


### **Tokenization Method**

This method iterates through the input text, character by character, and generates tokens based on the current character and its context. It recognizes various token types such as numbers, strings, variables, keywords, operators, and special characters.

Several methods are define to hadle different type of tokens:

**integer:** Scans and returns an integer from the input text.

**string:** Scans and returns a string enclosed in double quotes from the input text.

**variable:** Scans and returns a variable name consisting of alphanumeric characters.

**validate_email:** Validates if a given string represents a valid email address based on a predefined regular expression pattern (EMAIL_PATTERN).

**get_next_token:** The main method responsible for tokenization. It iterates through the input text, character by character, and determines the type of token based on the character encountered.

If conditions handle single character tokens such as '=', ',', '(', ')', '[', ']', and newline ('\n'). When any of these characters are encountered, the lexer creates the corresponding token and advances to the next character.

            if self.current_char == '(':
                self.advance()
                return Token(LPAREN, '(')

            if self.current_char == ')':
                self.advance()
                return Token(RPAREN, ')')

For example, the code above will handle the right and left paranthesis.

If the current character is a digit, it means the token is a number. The lexer calls **integer()** method to extract the entire integer and then creates a Token(NUMBER, value) with that integer value.

            if self.current_char.isdigit():
                return Token(NUMBER, self.integer())
                
If the current character is a double quote ('"'), it indicates the start of a string. The lexer advances to the next character and calls string() method to extract the entire string until it encounters another double quote.
After obtaining the string, it validates it using **validate_email()** method. If the string represents a valid email address, it creates a Token(STRING, value) with the email address value. Otherwise, it raises an exception for an invalid email format.

            if self.current_char == '"':
                self.advance()
                string_value = self.string()
                if self.validate_email(string_value):
                    return Token(STRING, string_value)
                else:
                    raise Exception('Invalid email format')
                    
If the current character is alphabetic (using isalpha()), it means the token is either a variable name or a keyword. The lexer calls variable() method to extract the entire variable name.
It then checks if the extracted token value (converted to lowercase) exists in the keywords list. If it does, it creates a Token(KEYWORD, value) with the lowercase keyword value. Otherwise, it creates a Token(VARIABLE, value) with the variable name.

            if self.current_char.isalpha():
                token_value = self.variable()
                if token_value.lower() in self.keywords:
                    return Token(KEYWORD, token_value.lower())
                else:
                    return Token(VARIABLE, token_value)
                    
If none of the above conditions are met, it implies an invalid character in the input text, and the lexer raises an exception (**self.error()**).
### **Example**

This part demonstrates how to use the lexer by tokenizing a sample email automation command represented as a multi-line string. It creates an instance of the Lexer class with the input text and then iterates through the generated tokens, printing each one until the end of the input is reached.

     lexer = Lexer(text)
     token = lexer.get_next_token()
     while token.type != EOF:
            print(token)
            token = lexer.get_next_token()

Example of the input that demonstrates the syntax of the DSL, including keywords like "send", "cc", "subject", "body", and their associated values.

     text = """
     send(
          email="dcretu@example.com",
          cc=["cc1@example.com", "cc2@example.com"],
          subject=Important Emai",
          body=LFA laboratory work 3
          )
    """

## Conclusions / Screenshots / Results

**Results**

The result represents the tokens generated by the lexer when tokenizing the input text. Each token consists of a type and a value. Each token type (KEYWORD, LPAREN, RPAREN, EQUALS, COMMA, STRING, etc.) represents a different category of lexical element in the input text.

 In my case, we can check the result if the email will be validated or not.

Results for correct email validation:

<img width="209" alt="image" src="https://github.com/nelldino/DSL-labs/assets/120444803/b3ec6ed2-3bad-47a4-b9a2-4333da7bd905">

The value of each token above represents the actual content of that element.

For example:

**Token(KEYWORD, 'send'):** Represents the keyword 'send'.

**Token(RBRACKET, ']'):** Represents the right square bracket ']', and so on for other tokens

The input for an incorrect email address (One of the email addresses does not correspond to the EMAIL_PATTERN. In the case below, it will not have the ”@” symbol)

     text = """
     send(
          email="dcretuexample.com",
          cc=["cc1@example.com", "cc2@example.com"],
          subject=Important Emai",
          body=LFA laboratory work 3
          )
    """


Results if the form for email address is incorrect:


<img width="666" alt="image" src="https://github.com/nelldino/DSL-labs/assets/120444803/3b2d2cdd-6d3f-45de-9e93-5ed513036135">



## References

[1] https://www.tutorialspoint.com/compiler_design/compiler_design_lexical_analysis.htm
